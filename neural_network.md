<!--------------------------------------------------------------------------------- Description -->
# Neural Network
    AI : Learning : Algorithm

<!--------------------------------------------------------------------------------- Architecture -->
<br><br>

## Architecture
MLP : Multi Layer Perceptron

[CNN : Convolutional Neural Network]

FNN  : Feedforward Neural Network

RNN  : Recurrent Neural Network

LSTM : Long Short-Term Memory

GRU  : Gated Recurrent Unit

GAN  : Generative Adversarial Networks

GNN  : Graph Neural Networks

AE   : Auto Encoder

SVM  : 

<!--------------------------------------------------------------------------------- Structure -->
<br><br>

## Structure
```
Input > Model > Output
```
```
Dense : Fully Connected Layer
Conv  : Convolutional Neural Network Layer
```
```
Layer     : Input Layers | Hidden Layers | Output Layers
Parameter : Input | Weight | Bias
Function  : Affine Transformation | Weightes Sum | Activation Functions | Loss Functions
```

<!--------------------------------------------------------------------------------- Activation Functions -->
<br><br>

## Activation Functions
<!-------------------------- Structure -->
Structure
```
Definition | Function | Derivative | Input | Output | Shape | Advantages | Disadvantages | Where it’s used today
```
<!-------------------------- Type -->
Type
```
Linear | Sigmoid | Tanh | ReLU | ELU | Leaky ReLU | Softmax
```

<!--------------------------------------------------------------------------------- Loss Function -->
<br><br>

## Loss Function
<!-------------------------- Structure -->
Structure
```
Definition |
```
<!-------------------------- Type -->
Type
```
Cross-Entropy
```

<!--------------------------------------------------------------------------------- Optimizer -->
<br><br>

## Optimizer
<!-------------------------- Structure -->
Structure
```
Definition | Function | Advantages | Disadvantages | Where it’s used today
```
<!-------------------------- Type -->
Type
```
Gradient Descent | Stochastic Gradient Descent | Momentum | AdaGrad | RMSProp | Adam
```

<!--------------------------------------------------------------------------------- Training -->
<br><br>

## Training
<!-------------------------- Dataset -->
### Dataset
```
Tarin
Validation
Test
```
<!-------------------------- Structure -->
### Structure
```
Feed forward
Error : Loss function | Cost function
Backpropagation
Epoch
Batch
```
<!-------------------------- Type -->
### Type
```
Supervised
Unsupervised
Semi-supervised
Self-supervised
Reinforcement Learning
Online/Incremental
Transfer/Fine-tuning
Few-shot / Zero-shot
Evolutionary methods
```
<!-------------------------- Problem -->
### Problem
```
Vanishing Gradient
Learning rate
```

<!--------------------------------------------------------------------------------- Input -->
<br><br>

## Input
```
Representation learning
Type
Lable
Cleaning
Normalization
Tokenize
Limitation
```

<!--------------------------------------------------------------------------------- Output -->
<br><br>

## Output
```
Predict
Nois
Output 
Evaluation criteria : Accuracy, Precision, Recall, F1
Feedback & Iteration
Format : Probabilities, Text, Labels, h5, pt, onnx, joblib
```

<!--------------------------------------------------------------------------------- Subject -->
<br><br>

## Subject
```
Fine-tuning
```

<!--------------------------------------------------------------------------------- App -->
<br><br>

## App
https://github.com/lutzroeder/netron

<!--------------------------------------------------------------------------------- Note -->
<br><br>

## Note
```
Activation Functions Must be differentiable
```

<!--------------------------------------------------------------------------------- Extra -->
<br><br>

## Extra
```
seq2seq
Transformer
Train
Foundation model
Fine-Tuning
Token
Temperature
Matching
Masking
RLHF
Reward model
Medium | Abstraction | Context | Statefulness
Token
Gradient Descent
Gradient
```

<!--------------------------------------------------------------------------------- Links -->
[CNN : Convolutional Neural Network]: https://github.com/kashanimorteza/ai_document/tree/main/cnn.md



